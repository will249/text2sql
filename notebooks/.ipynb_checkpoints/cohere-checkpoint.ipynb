{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8b0e6f6-e018-4a7d-b863-da3e3753427e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet langchain langchain_cohere langchain_community langchain-openai tiktoken langchainhub chromadb langgraph gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9ce39f-8525-428f-ae80-b61c37d14c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_cohere import CohereEmbeddings, ChatCohere\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e6d3c47-68a5-4172-89af-afc1f2ffd3f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_local = \"No\"\n",
    "generator_model = \"cohere\" # cohere | openai\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702d69aa-3959-4311-bca0-98adf1a5d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ollama pull gemma:2b\n",
    "local_llm = \"gemma:2b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a07d6-c23a-47e6-b0cf-1de71b63f807",
   "metadata": {},
   "source": [
    "### Get embeddings from SQL metadata\n",
    "\n",
    "Using document comprised of SQL CREATE queries and comments\n",
    "https://arxiv.org/pdf/2204.00498.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf040628-d07e-400f-aa07-77286e008997",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"./seed_db.sql\")\n",
    "documents = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=200\n",
    ")\n",
    "all_splits = splitter.split_documents(documents)\n",
    "\n",
    "# Embed and index\n",
    "if run_local == \"Yes\":\n",
    "    embedding = GPT4AllEmbeddings()\n",
    "else:\n",
    "    embedding = CohereEmbeddings()\n",
    "\n",
    "\n",
    "# Index\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    collection_name=\"db-metadata-embeddings\",\n",
    "    embedding=embedding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb65765-7623-40fc-9287-6d3404905c45",
   "metadata": {},
   "source": [
    "### Define graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f58971e-da8e-4e2f-8341-ddc78becd133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        optimised_question: question optimised for vector search\n",
    "        rewrite_question: str\n",
    "        generation: LLM generation\n",
    "        documents: list of documents \n",
    "    \"\"\"\n",
    "    question : str\n",
    "    optimised_question: str\n",
    "    rewrite_needed: str\n",
    "    generation : str\n",
    "    documents : List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648f25a-8088-4ea2-b7c2-c0d2136f072e",
   "metadata": {},
   "source": [
    "### Define graph nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81fb52b-cef5-491e-9014-1d5970cfef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval  (node)\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"---RETRIEVING---\")\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    if state[\"rewrite_needed\"] == 'yes':\n",
    "        question = state[\"optimised_question\"]\n",
    "    else: \n",
    "        question = state[\"question\"]\n",
    "\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca83acd-022c-4165-b2c7-11d4e06caa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/langchain-aws-service/lib/python3.9/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "# Retrieval Grader (node)\n",
    "\n",
    "# Pydantic object for structured output\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "preamble = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "if run_local == \"Yes\":\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "else:\n",
    "    llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments, preamble=preamble)\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def grade_documents(state):\n",
    "    \n",
    "    retrieval_grader = grade_prompt | structured_llm_grader\n",
    "    \n",
    "    print(\"---CHECKING DOCUMENT RELEVANCE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # Filter for relevant docs\n",
    "    filtered_docs = []\n",
    "    rewrite_needed = 'no'\n",
    "    for i, d in enumerate(documents):\n",
    "        grade = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content}).binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(f\"------grader: document {i+1} relevant---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(f\"------grader: document {i+1} not relevant---\")\n",
    "    \n",
    "    if len(filtered_docs) == 0:\n",
    "        rewrite_needed = \"yes\"\n",
    "    return {\"documents\": filtered_docs, \"rewrite_needed\": rewrite_needed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6dacdd2-21d2-4f90-ac88-585215bd9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewrite question to be more suitable for similarity search (node)\n",
    "\n",
    "preamble = \"\"\"You are an expert at re-writing questions so that they are optimized \\n \n",
    "     for vectorstore retrieval. Take the original question and create an improved question. \\n\n",
    "\"\"\"\n",
    "\n",
    "if run_local == \"Yes\":\n",
    "    llm = ChatOllama(model=local_llm, temperature=0)\n",
    "else:\n",
    "    llm = ChatCohere(model=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "# Prompt \n",
    "rewrite_prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            f\"Original question: {x['question']} \\nNew question: \",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "question_rewriter = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "def rewrite_question(state):\n",
    "    print(\"---REWRITING QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    optimised_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"optimised_question\": optimised_question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb22f0e-7da1-42bd-83bd-0978846fb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide whether to generate or rewrite question (conditional edge)\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    rewrite_needed = state[\"rewrite_needed\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    if rewrite_needed == \"yes\":\n",
    "        print(\"------decision: No documents relevant to question, rewriting---\")\n",
    "        return \"rewrite\"\n",
    "    else:\n",
    "        print(\"------decision: Generate---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05bfdf43-30ff-450c-8286-65ae98d07c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate! (node)\n",
    "\n",
    "if generator_model == 'openai':\n",
    "    llm = ChatOpenAI(model=\"gpt-4\", temperature=0) \n",
    "else:\n",
    "    llm = ChatCohere(model_name=\"command-r\", temperature=0)\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are a postgres expert. \n",
    "    Use the following pieces of retrieved context (relevant DB tables) to generate a syntactically correct SQL query that answers the user's question.\n",
    "    When creating location-based queries, use tables containing GIS objects as this permits highly optimised geo-spatial querying.\n",
    "    Only return the SQL query, with no explanation or preamble.\n",
    "    If you can't formulate a valid query, just say that you don't know. \\n \n",
    "    Here is the retrieved context: \\n\\n {documents} \\n\\n\n",
    "    Here is the user question: {question} \\n\"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "generation_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---GENERATING---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    if not isinstance(documents, list):\n",
    "      documents = [documents]\n",
    "\n",
    "    generation = generation_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be50729f-04dd-4357-ac38-41559039deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval (conditional edge)\n",
    "\n",
    "if run_local == \"Yes\":\n",
    "    llm = ChatOllama(model=local_llm, format=\"json\", temperature=0)\n",
    "else:\n",
    "    llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "\n",
    "class ValidityEval(BaseModel):\n",
    "    \"\"\"Binary score for validity of generated SQL query.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Generated query is valid SQL, 'yes' or 'no'\")\n",
    "\n",
    "preamble = \"\"\"You are a grader assessing the validity of a SQL query generated from text. If the query parses\n",
    "as valid SQL, give a binary score of 'yes', otherwise give a binary score 'no'. \n",
    "Provide the binary score as JSON with a single key 'score' and no explanation.\"\"\"\n",
    "structured_evaluator = llm.with_structured_output(ValidityEval, preamble=preamble)\n",
    "validity_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Generated SQL query: \\n\\n {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "validity_evaluator = validity_prompt | structured_evaluator\n",
    "class AnswerEval(BaseModel):\n",
    "    \"\"\"Binary score to assess whether generated SQL query answers the user's question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Generated query addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "preamble = \"\"\"You are a SQL expert that can determine whether a SQL query addresses / resolves a question \\n\n",
    "Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question. \\n\n",
    "Provide the binary score as JSON with a single key 'score' and no explanation.\"\"\"\n",
    "structured_evaluator = llm.with_structured_output(AnswerEval, preamble=preamble)\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n Generated SQL query: \\n\\n {generation}\"),\n",
    "    ]\n",
    ")\n",
    "answer_evaluator = answer_prompt | structured_evaluator\n",
    "\n",
    "class IdempEval(BaseModel):\n",
    "    \"\"\"Binary score for whether SQL query is idempotent.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Generated query is idempotent, 'yes' or 'no'\")\n",
    "\n",
    "preamble = \"\"\"You are a grader assessing whether a generated SQL query permanently \\n\n",
    "alters the database. (i.e. whether it deletes, inserts, or updates entries in database tables, or deletes tables). \\n\n",
    "Permit JOINS and string functions that only temporarily alter the data returned by the query). \\n\n",
    "If the query does indeed leave the data unchanged, give a binary score of 'yes', otherwise give a binary score 'no'. \n",
    "Provide the binary score as JSON with a single key 'score' and no explanation.\"\"\"\n",
    "\n",
    "structured_evaluator = llm.with_structured_output(IdempEval, preamble=preamble)\n",
    "\n",
    "idemp_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Generated SQL query: \\n\\n {generation}\"),\n",
    "    ]\n",
    ")\n",
    "idemp_evaluator = idemp_prompt | structured_evaluator\n",
    "\n",
    "def evaluate(state):\n",
    "    \"\"\"\n",
    "    Determines whether generation answers question, is valid SQL, and is idempotent.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---EVALUATING GENERATION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    if validity_evaluator.invoke({\"generation\": generation}).binary_score == \"yes\":\n",
    "        print(\"------decision: query is valid sql---\")\n",
    "    else: \n",
    "        print(\"------decision: query is not valid sql---\")\n",
    "        print(generation)\n",
    "        return \"fail\"\n",
    "        \n",
    "    if answer_evaluator.invoke({\"question\": question, \"generation\": generation}).binary_score == \"yes\":\n",
    "        print(\"------decision: query answers user's question---\")\n",
    "    else: \n",
    "        print(\"------decision: query does not answer user's question---\")\n",
    "        print(generation)\n",
    "        return \"fail\"\n",
    "        \n",
    "    if idemp_evaluator.invoke({\"generation\": generation}).binary_score == \"yes\":\n",
    "        print(\"------decision: query is idempotent---\")\n",
    "    else: \n",
    "        print(\"------decision: query is not idempotent---\")\n",
    "        print(generation)\n",
    "        return \"fail\"\n",
    "\n",
    "    return \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79733df3-8c6a-45c0-9bdc-112b4ce2e813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"rewrite_question\", rewrite_question)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"rewrite\": \"rewrite_question\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"rewrite_question\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    evaluate,\n",
    "    {\n",
    "        \"fail\": \"generate\", # Failed eval => re-generate \n",
    "        \"success\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fb797b6-13bc-4a09-b2fb-80cc9b5c7a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVING---\n",
      "\n",
      "---CHECKING DOCUMENT RELEVANCE---\n",
      "------grader: document 1 not relevant---\n",
      "------grader: document 2 relevant---\n",
      "------grader: document 3 not relevant---\n",
      "------grader: document 4 not relevant---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "------decision: Generate---\n",
      "\n",
      "---GENERATING---\n",
      "---EVALUATING GENERATION---\n",
      "------decision: query is valid sql---\n",
      "------decision: query answers user's question---\n",
      "------decision: query is idempotent---\n",
      "\n",
      "```sql\n",
      "SELECT tail_number\n",
      "FROM aircraft_assets\n",
      "WHERE model = '787';\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"Get tail numbers of all aircraft with model '787'\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(\"\")\n",
    "\n",
    "print(output['generate']['generation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "236c1c14-ce28-4c96-9fdc-4942d754710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVING---\n",
      "(Node: 'retrieve')\n",
      "---CHECKING DOCUMENT RELEVANCE---\n",
      "------grader: document 1 relevant---\n",
      "------grader: document 2 relevant---\n",
      "------grader: document 3 not relevant---\n",
      "------grader: document 4 not relevant---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "------decision: Generate---\n",
      "(Node: 'grade_documents')\n",
      "---GENERATING---\n",
      "---EVALUATING GENERATION---\n",
      "------decision: query is valid sql---\n",
      "------decision: query answers user's question---\n",
      "------decision: query is idempotent---\n",
      "(Node: 'generate')\n",
      "```sql\n",
      "SELECT SUM(duration_hours) AS total_hours\n",
      "FROM aircraft_historical_flights\n",
      "WHERE\n",
      "    tail_number = 'AB123'\n",
      "    AND EXISTS (\n",
      "        SELECT 1\n",
      "        FROM aircraft_position_gis\n",
      "        WHERE\n",
      "            aircraft_position_gis.flight_id = aircraft_historical_flights.flight_id\n",
      "            AND ST_Within(aircraft_position_gis.position, (\n",
      "                SELECT region\n",
      "                FROM regions_of_interest\n",
      "                WHERE LOWER(name) = 'slovakia'\n",
      "            ))\n",
      "    );\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"Calculate how much time aircraft AB123 spent flying over the region of interest 'Slovakia'\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        print(f\"(Node: '{key}')\")\n",
    "\n",
    "print(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c28f2-6f75-4bf4-86bb-a145a7b6a466",
   "metadata": {},
   "source": [
    "### Very basic benchmarking with LangSmith\n",
    "ProjectID: abe556aa-fe68-4b03-a365-7791b5c9f263\n",
    "\n",
    "**Simple query:**\n",
    "\n",
    "| Generation Model | Speed | Cost ($) | Quality of Query |\n",
    "| ----------- | ----------- | --------- | --------- |\n",
    "| Cohere cmdR | 1s (6.6s total) | 0.00083 | perfect |\n",
    "| GPT4 | 1.3s (6.9s total) | 0.014 | perfect |\n",
    "\n",
    "\n",
    "**Complex query:**\n",
    "\n",
    "| Generation Model | Speed | Cost ($) | Quality of Query |\n",
    "| ----------- | ----------- | --------- | --------- |\n",
    "| Cohere cmdR | 3.5s (8.9s total) | 0.0013 | missed details in time calculation |\n",
    "| GPT4 | 4.6s (10.2s total) | 0.031 | perfect |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95274824-2580-4613-beec-23b72170677f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
