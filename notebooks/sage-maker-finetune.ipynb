{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b9dd6-b07e-4147-ba1a-136319f1ae2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install \"sagemaker>=2.140.0\" \"transformers==4.26.1\" \"datasets[s3]==2.10.1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ae56f-fe14-4401-9ffe-9ea874ccb56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch tensorboard --quiet\n",
    "!pip install  --upgrade transformers datasets accelerate evaluate bitsandbytes --quiet\n",
    "!pip install peft --quiet\n",
    "!pip install datasets trl ninja packaging --quiet\n",
    "!pip install diffusers safetensors  --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac297f-44cf-45fb-9391-5fa458551322",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "access_token = os.getenv(\"\")\n",
    "login(\n",
    " token=access_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b588a684-3664-4eb4-8b93-31d70f734cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3e887-b674-4142-8d7d-e8a977a01b9b",
   "metadata": {},
   "source": [
    "## Create dataset\n",
    "\n",
    "Using [sql-create-context]('https://huggingface.co/datasets/b-mc2/sql-create-context) dataset. Specifically built for text to sql using CREATE statements in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d70ebb-8d88-42a9-b3ae-fbc073bca845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Model expects conversation input:\n",
    "system_message = \"\"\"You are an exert text-to-SQL query translator. Users will ask you questions in English and you will generate a syntactically correct SQL query based on the provided SCHEMA.\n",
    "SCHEMA:\n",
    "{schema}\"\"\"\n",
    "\n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message.format(schema=sample[\"context\"])},\n",
    "      {\"role\": \"user\", \"content\": sample[\"question\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"answer\"]}\n",
    "    ]\n",
    "  }\n",
    "\n",
    "# Load subset of dataset from the huggingface hub\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "dataset = dataset.shuffle().select(range(20000))\n",
    "\n",
    "# Convert dataset to messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)\n",
    "dataset = dataset.train_test_split(test_size=2500/17500)\n",
    "\n",
    "print(dataset[\"train\"][1][\"messages\"])\n",
    "\n",
    "dataset[\"train\"].to_json(\"train_dataset.json\", orient=\"records\")\n",
    "dataset[\"test\"].to_json(\"test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a5a9f-cde1-4396-8f81-056e4742a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c6cc5-3818-43d7-966f-c6a4a25cb561",
   "metadata": {},
   "source": [
    "## Benchmarking cmdR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b062c1-06c1-40c0-bcb4-dd7437ca73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "model_id = \"CohereForAI/c4ai-command-r-v01-4bit\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# load into pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3596a3-6c70-4ebd-958a-09013d059133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test_dataset.json\", split=\"train\")\n",
    "\n",
    "def test(sample):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(sample[\"messages\"][1:2], tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, temperature=0, top_k=50, top_p=0.95, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    "    pred = outputs[0]['generated_text'][len(prompt):].strip()\n",
    "    if pred == sample[\"messages\"][2][\"content\"]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "all_preds = []\n",
    "n_test = 2500\n",
    "\n",
    "for s in tqdm(test_dataset.shuffle().select(range(n_test))):\n",
    "    all_preds.append(test(s))\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = sum(all_preds)/len(all_preds)\n",
    "\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd797680-1faa-4712-8d6a-7ec89bad903c",
   "metadata": {},
   "source": [
    "## Finetuning Mistral 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950af08-2419-458c-866a-3ca1cee65f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# 4-bit quantization using BitsAndBytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "# Ensure using openAI chat format\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe2741-2a1a-444f-8fe2-5bf61cb0c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from peft import LoraConfig\n",
    "# PEFT = parameter efficient tuning, of which LoRA is a particular method\n",
    "\n",
    "# config from QLoRA paper (quantization + LoRA)\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"walexand3r/Mistral-7B-v0.2-text-to-sql\",  # Save to hugging face   \n",
    "    num_train_epochs=3,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=2,          # number of steps before performing a backwards pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02932bb-e865-4440-b7c3-9fc424b11d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "# TRL is huggingfaces's transformer reinforcement learning framework\n",
    "# SFT is a supervised finetuning package\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=3072,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a3047-e74d-4598-8056-912ca68c1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cc311-7fd3-40d1-b073-65dd925de187",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Repeat benchmarking for finetuned model\n",
    "- Save model to s3 rather than huggingface\n",
    "- Convert to script and use SageMaker experiments to run managed training run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
